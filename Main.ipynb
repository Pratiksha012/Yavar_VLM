{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86a2c58f-2c30-42bb-a48e-d81078dd096c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pratiksha/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=11.99s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.74s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pratiksha/Library/Python/3.9/lib/python/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/Users/pratiksha/Library/Python/3.9/lib/python/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/Users/pratiksha/Library/Python/3.9/lib/python/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import clip\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import numpy as np\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from rouge_score import rouge_scorer\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from pycocotools.coco import COCO\n",
    "import requests\n",
    "import zipfile\n",
    "import shutil\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(filename='captioning_log.txt', level=logging.INFO)\n",
    "\n",
    "# Directories\n",
    "IMG_FOLDER = \"img_folder\"\n",
    "METADATA_FOLDER = \"metadata_folder\"\n",
    "OUTPUT_FOLDER = \"output_folder\"\n",
    "DATASET_FOLDER = \"coco_dataset\"\n",
    "os.makedirs(IMG_FOLDER, exist_ok=True)\n",
    "os.makedirs(METADATA_FOLDER, exist_ok=True)\n",
    "os.makedirs(OUTPUT_FOLDER, exist_ok=True)\n",
    "os.makedirs(DATASET_FOLDER, exist_ok=True)\n",
    "\n",
    "# Load CLIP model and processor\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-large-patch14-336\").to(device)\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-large-patch14-336\")\n",
    "\n",
    "# Download and prepare COCO dataset\n",
    "def download_coco_dataset():\n",
    "    logging.info(\"Downloading COCO 2017 dataset...\")\n",
    "    # Download a small subset of COCO images and annotations\n",
    "    img_url = \"http://images.cocodataset.org/train2017/0000000000*.jpg\"  # Example subset\n",
    "    ann_url = \"http://images.cocodataset.org/annotations/annotations_trainval2017.zip\"\n",
    "    \n",
    "    # Download annotations\n",
    "    ann_path = os.path.join(DATASET_FOLDER, \"annotations.zip\")\n",
    "    if not os.path.exists(ann_path):\n",
    "        response = requests.get(ann_url)\n",
    "        with open(ann_path, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "        with zipfile.ZipFile(ann_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall(DATASET_FOLDER)\n",
    "    \n",
    "    # Simulate downloading a few images (replace with actual download logic)\n",
    "    # For demo, assume images are in DATASET_FOLDER/train2017/\n",
    "    os.makedirs(os.path.join(DATASET_FOLDER, \"train2017\"), exist_ok=True)\n",
    "    # Placeholder: Copy a few sample images to img_folder for testing\n",
    "    # In practice, download images using img_url or use existing images\n",
    "\n",
    "# Fine-tune CLIP model\n",
    "def fine_tune_model(model, processor, dataset_path=DATASET_FOLDER):\n",
    "    logging.info(\"Fine-tuning CLIP model with COCO dataset...\")\n",
    "    coco = COCO(os.path.join(dataset_path, \"annotations/instances_train2017.json\"))\n",
    "    caption_file = os.path.join(dataset_path, \"annotations/captions_train2017.json\")\n",
    "    coco_caps = COCO(caption_file)\n",
    "    \n",
    "    # Get image IDs\n",
    "    img_ids = coco.getImgIds()\n",
    "    train_data = []\n",
    "    \n",
    "    for img_id in img_ids[:1000]:  # Limit to 1000 images for demo\n",
    "        img_info = coco.loadImgs(img_id)[0]\n",
    "        img_path = os.path.join(dataset_path, \"train2017\", img_info['file_name'])\n",
    "        ann_ids = coco_caps.getAnnIds(imgIds=img_id)\n",
    "        captions = [ann['caption'] for ann in coco_caps.loadAnns(ann_ids)]\n",
    "        if os.path.exists(img_path) and captions:\n",
    "            train_data.append((img_path, captions[0]))  # Use first caption\n",
    "    \n",
    "    # Fine-tuning loop\n",
    "    model.train()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "    for epoch in range(2):  # Limited epochs for demo\n",
    "        for img_path, caption in train_data:\n",
    "            try:\n",
    "                image = Image.open(img_path).convert(\"RGB\")\n",
    "                inputs = processor(text=[caption], images=image, return_tensors=\"pt\", padding=True).to(device)\n",
    "                outputs = model(**inputs)\n",
    "                loss = outputs.loss\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error in fine-tuning for {img_path}: {e}\")\n",
    "    \n",
    "    model.eval()\n",
    "    logging.info(\"Fine-tuning completed.\")\n",
    "    return model, processor\n",
    "\n",
    "# Preprocess image\n",
    "def preprocess_image(image_path):\n",
    "    try:\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        image = image.resize((336, 336))\n",
    "        image_np = np.array(image).astype(np.float32) / 255.0\n",
    "        return Image.fromarray((image_np * 255).astype(np.uint8))\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error preprocessing image {image_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Read metadata\n",
    "def read_metadata(metadata_path):\n",
    "    metadata = {\n",
    "        \"section_header\": None,\n",
    "        \"above_text\": None,\n",
    "        \"caption\": None,\n",
    "        \"picture_id\": None,\n",
    "        \"footnote\": None,\n",
    "        \"below_text\": None\n",
    "    }\n",
    "    try:\n",
    "        with open(metadata_path, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "            for line in lines:\n",
    "                key, value = line.strip().split(\": \", 1)\n",
    "                metadata[key] = value if value != \"null\" else None\n",
    "        return metadata\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error reading metadata {metadata_path}: {e}\")\n",
    "        return metadata\n",
    "\n",
    "# Generate captions\n",
    "def generate_captions(image, metadata, model, processor):\n",
    "    text_inputs = []\n",
    "    if metadata[\"section_header\"]:\n",
    "        text_inputs.append(f\"Section: {metadata['section_header']}\")\n",
    "    if metadata[\"above_text\"]:\n",
    "        text_inputs.append(f\"Above: {metadata['above_text']}\")\n",
    "    if metadata[\"below_text\"]:\n",
    "        text_inputs.append(f\"Below: {metadata['below_text']}\")\n",
    "    if metadata[\"footnote\"]:\n",
    "        text_inputs.append(f\"Footnote: {metadata['footnote']}\")\n",
    "    context = \" \".join(text_inputs)\n",
    "    \n",
    "    inputs = processor(text=[context], images=image, return_tensors=\"pt\", padding=True).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        image_embeds = outputs.image_embeds\n",
    "        text_embeds = outputs.text_embeds\n",
    "    \n",
    "    # Simulated caption generation (using context)\n",
    "    concise_caption = f\"Summary of {context or 'image content'}.\"\n",
    "    detailed_caption = f\"Detailed description: {context or 'image content'}.\"\n",
    "    \n",
    "    # Confidence scores (based on cosine similarity)\n",
    "    cosine_sim = torch.cosine_similarity(image_embeds, text_embeds).item()\n",
    "    concise_confidence = min(cosine_sim, 0.95)\n",
    "    detailed_confidence = min(cosine_sim * 0.9, 0.90)\n",
    "    \n",
    "    # Check consistency\n",
    "    if metadata[\"section_header\"] and metadata[\"section_header\"].lower() not in detailed_caption.lower():\n",
    "        detailed_confidence *= 0.5\n",
    "        logging.warning(f\"Low confidence for detailed caption due to metadata mismatch: {metadata['section_header']}\")\n",
    "    \n",
    "    return {\n",
    "        \"concise\": {\"caption\": concise_caption, \"confidence\": concise_confidence},\n",
    "        \"detailed\": {\"caption\": detailed_caption, \"confidence\": detailed_confidence}\n",
    "    }\n",
    "\n",
    "# Overlay captions on image\n",
    "def overlay_captions(image, captions):\n",
    "    draw = ImageDraw.Draw(image)\n",
    "    try:\n",
    "        font = ImageFont.truetype(\"arial.ttf\", 20)\n",
    "    except:\n",
    "        font = ImageFont.load_default()\n",
    "    \n",
    "    y_concise = image.height - 60\n",
    "    y_detailed = image.height - 30\n",
    "    \n",
    "    concise_text = captions[\"concise\"][\"caption\"]\n",
    "    concise_confidence = captions[\"concise\"][\"confidence\"]\n",
    "    draw.text((10, y_concise), concise_text, fill=\"blue\", font=font)\n",
    "    if concise_confidence < 0.7:\n",
    "        draw.line((10, y_concise + 20, 10 + len(concise_text) * 10, y_concise + 20), fill=\"blue\")\n",
    "    \n",
    "    detailed_text = captions[\"detailed\"][\"caption\"]\n",
    "    detailed_confidence = captions[\"detailed\"][\"confidence\"]\n",
    "    draw.text((10, y_detailed), detailed_text, fill=\"red\", font=font)\n",
    "    if detailed_confidence < 0.7:\n",
    "        draw.line((10, y_detailed + 20, 10 + len(detailed_text) * 10, y_detailed + 20), fill=\"red\")\n",
    "    \n",
    "    return image\n",
    "\n",
    "# Evaluate captions\n",
    "def evaluate_captions(generated, reference):\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\n",
    "    bleu_score = sentence_bleu([reference.split()], generated.split())\n",
    "    rouge_scores = scorer.score(reference, generated)\n",
    "    return {\"bleu\": bleu_score, \"rouge1\": rouge_scores['rouge1'].fmeasure, \"rougeL\": rouge_scores['rougeL'].fmeasure}\n",
    "\n",
    "# Main processing loop\n",
    "def process_images():\n",
    "    captions_output = {}\n",
    "    \n",
    "    for img_file in os.listdir(IMG_FOLDER):\n",
    "        if img_file.endswith(('.png', '.jpg', '.jpeg')):\n",
    "            img_path = os.path.join(IMG_FOLDER, img_file)\n",
    "            metadata_path = os.path.join(METADATA_FOLDER, img_file.rsplit('.', 1)[0] + '.txt')\n",
    "            \n",
    "            image = preprocess_image(img_path)\n",
    "            if not image:\n",
    "                continue\n",
    "                \n",
    "            metadata = read_metadata(metadata_path)\n",
    "            \n",
    "            captions = generate_captions(image, metadata, model, processor)\n",
    "            \n",
    "            annotated_image = overlay_captions(image.copy(), captions)\n",
    "            \n",
    "            output_img_path = os.path.join(OUTPUT_FOLDER, f\"annotated_{img_file}\")\n",
    "            annotated_image.save(output_img_path)\n",
    "            \n",
    "            eval_scores = {}\n",
    "            if metadata[\"caption\"]:\n",
    "                eval_scores[\"concise\"] = evaluate_captions(captions[\"concise\"][\"caption\"], metadata[\"caption\"])\n",
    "                eval_scores[\"detailed\"] = evaluate_captions(captions[\"detailed\"][\"caption\"], metadata[\"caption\"])\n",
    "            \n",
    "            captions_output[img_file] = {\n",
    "                \"concise_caption\": captions[\"concise\"],\n",
    "                \"detailed_caption\": captions[\"detailed\"],\n",
    "                \"evaluation\": eval_scores\n",
    "            }\n",
    "    \n",
    "    with open(os.path.join(OUTPUT_FOLDER, \"captions.json\"), 'w') as f:\n",
    "        json.dump(captions_output, f, indent=4)\n",
    "    \n",
    "    logging.info(\"Processing completed successfully.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Download and prepare dataset\n",
    "    download_coco_dataset()\n",
    "    \n",
    "    # Fine-tune model\n",
    "    model, processor = fine_tune_model(model, processor)\n",
    "    \n",
    "    # Process images\n",
    "    process_images()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69877f54-f451-490f-965b-e17aab972d66",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
